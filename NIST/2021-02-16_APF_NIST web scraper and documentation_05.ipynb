{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NIST Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code documents the methods of combining and organizing the scraped data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The web scraping was conducted through the following code. Increments of 27 reference ID URLs were scraped to maintain accuracy of the scraper while executing a timely web scrape.\n",
    "\n",
    "The above scraping protocols were refined and consolidated. The final dataframe consists of the desired columns from all components of the NIST website and the static HTML file. A 30-second time delay was introduced to the end of each scraping loop, which theoretically allows the NIST server to recover and critically improves the accuracy of the scraped values to an acceptable threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Statements\n",
    "from bs4 import BeautifulSoup\n",
    "import requests #Pulling webpages\n",
    "import pandas\n",
    "import datetime\n",
    "import numpy\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "\n",
    "# defining the website\n",
    "root_url = \"https://randr.nist.gov/enzyme/DataDetails.aspx?ID=\"\n",
    "end_url = \"&finalterm=&data=enzyme\"\n",
    "#===========================================================================================================================\n",
    "\n",
    "# identify the table and rows of pertinent data\n",
    "bs = BeautifulSoup(open('Enzyme Thermodynamic Database.html'), 'lxml')\n",
    "table = bs.find(\"table\", attrs = {'id': 'MainBody_gvSearch'})\n",
    "body = table.find_all(\"tr\")\n",
    "#print(body[1])\n",
    "\n",
    "# defining the boundaries of the dataframe section\n",
    "index_range = 12000\n",
    "index_count = 0\n",
    "loop_count = 0 \n",
    "output_loop = 1\n",
    "lower_bound = 1 #math.floor(1*len(body)/50)\n",
    "upper_bound = math.floor(1*len(body))\n",
    "\n",
    "# loop through the enzyme id values \n",
    "name_iteration = 0\n",
    "enzyme_iteration = 0\n",
    "for row in range(lower_bound,upper_bound):   \n",
    "    # parsing the reaction names and strings   \n",
    "    enzyme_name = body[row].find('span', attrs = {'id': 'MainBody_gvSearch_lblEnzyme_%s' %(name_iteration)}).text\n",
    "    reaction = body[row].find('span', attrs = {'id': 'MainBody_gvSearch_lblReaction_%s' %(name_iteration)}).text\n",
    "    id_value = body[row].find(\"a\").text\n",
    "    name_iteration += 1\n",
    "    \n",
    "    # defining the soup\n",
    "    total_url = root_url + id_value + end_url\n",
    "    soup = requests.get(total_url).text\n",
    "    bs = BeautifulSoup(soup, 'lxml')\n",
    "    \n",
    "    # scrape the table and header information\n",
    "    tables1 = bs.find_all(\"table\", attrs = {\"id\": \"MainBody_extraData\"})\n",
    "    print(id_value, '\\t: ', loop_count, '\\t, ', len(tables1))\n",
    "    if len(tables1) != 1:\n",
    "        continue\n",
    "\n",
    "    body1 = tables1[0].find_all(\"tr\")\n",
    "    body_rows1 = body1[1:]\n",
    "    heads = body1[0]\n",
    "\n",
    "    headings = ['Enzyme:', 'Reaction:']\n",
    "    for head in heads.find_all(\"th\"):\n",
    "        head = (head.text).rstrip(\"\\n\")\n",
    "        headings.append(head)\n",
    "        \n",
    "    #print(headings)\n",
    "\n",
    "    total_rows = []\n",
    "    for row_number in range(len(body_rows1)):\n",
    "        each_row = [enzyme_name, reaction]\n",
    "        for row_element in body_rows1[row_number].find_all(\"td\"):\n",
    "            row_refined = re.sub(\"(\\xa0)|(\\n)|,\",\"\",row_element.text)\n",
    "            each_row.append(row_refined)\n",
    "        total_rows.append(each_row)\n",
    "        \n",
    "    #print(total_rows)\n",
    "    \n",
    "    # create a dataframe\n",
    "    index_list_body = range(index_count+lower_bound-1, len(body_rows1)+index_count+lower_bound-1)\n",
    "    bs_dataframe_table1 = pandas.DataFrame(data = total_rows, columns = headings, index = index_list_body)\n",
    "    bs_dataframe_table1.drop(bs_dataframe_table1.columns[len(bs_dataframe_table1.columns)-1], axis=1, inplace=True)\n",
    "    '''display(bs_dataframe_table1)'''\n",
    "    \n",
    "#===========================================================================================================================\n",
    "\n",
    "    # scrape additional information \n",
    "    tables = bs.find_all(\"table\", attrs={\"id\": \"MainBody_DataList1\"})\n",
    "    if len(tables) == 0:\n",
    "        continue\n",
    "        print('Failed reference ID: ', id_value)\n",
    "        \n",
    "    body2 = tables[0].find_all(\"tr\")\n",
    "    body_rows2 = body2[1:]\n",
    "\n",
    "    each_row2 = []\n",
    "    for row in range(len(body_rows2)):\n",
    "        for row_element in body_rows2[row].find_all(\"td\"):\n",
    "            #print('row element: ', row_element)\n",
    "            row_refined2 = re.sub(\"(\\xa0)|(\\n)|,\",\"\",row_element.text)\n",
    "            #print('row refined', row_refined2)\n",
    "            each_row2.append(row_refined2)\n",
    "\n",
    "    information_entries_list = []\n",
    "    information_values_list = []\n",
    "    column_count = 0\n",
    "    for i, element in enumerate(each_row2):\n",
    "        if i == 0 or i % 2 == 0:\n",
    "            information_entries_list.append(element)\n",
    "            column_count += 1\n",
    "        else:\n",
    "            information_values_list.append(element)\n",
    "            column_count += 1\n",
    "    column_count /= 2\n",
    "\n",
    "    # create the dataframe and refine the columns\n",
    "    index_list_reference = range(index_count+lower_bound-1, index_count+1+lower_bound-1)\n",
    "    bs_dataframe_pretable2 = pandas.DataFrame(data = [information_values_list], columns = information_entries_list, index = index_list_reference)\n",
    "    bs_dataframe_pretable2.drop(bs_dataframe_pretable2.columns[len(bs_dataframe_pretable2.columns)-2], axis=1, inplace=True)\n",
    "    bs_dataframe_pretable2.drop(bs_dataframe_pretable2.columns[len(bs_dataframe_pretable2.columns)-1], axis=1, inplace=True)\n",
    "    '''display(bs_dataframe_pretable2)'''\n",
    "    \n",
    "#===========================================================================================================================\n",
    "\n",
    "    # merge the dataframes of this loop\n",
    "    this_dataframe = bs_dataframe_table1.join(bs_dataframe_pretable2)\n",
    "    this_dataframe.index.name = 'index'\n",
    "    \n",
    "    # iteratively coalesce the new dataframe into the old dataframe \n",
    "    if loop_count == 0:\n",
    "        old_dataframe = this_dataframe\n",
    "        old_dataframe.index.name = 'index'\n",
    "        \n",
    "    elif loop_count > 0:\n",
    "        these_columns = []\n",
    "        for column in this_dataframe:\n",
    "            these_columns.append(column)\n",
    "            \n",
    "        old_columns = []\n",
    "        for existing_column in old_dataframe:\n",
    "            old_columns.append(existing_column)\n",
    "            \n",
    "        common_columns = list(set(these_columns).intersection(old_columns))\n",
    "        \n",
    "        # amalgamate the dataframe with the pre-existing dataframe\n",
    "        current_dataframe = old_dataframe.merge(this_dataframe, on = common_columns, how = 'outer')\n",
    "        old_dataframe = current_dataframe  \n",
    "        \n",
    "    # amalgamate the dataframe with the pre-existing dataframe\n",
    "    index_count += len(body_rows1)\n",
    "    \n",
    "    time_delay = 0\n",
    "    time.sleep(time_delay)\n",
    "    \n",
    "    max_referenes_per_csv = 0\n",
    "    if loop_count == max_referenes_per_csv:\n",
    "        id_value = re.sub('(/)', '-', id_value)\n",
    "        output = './individual scraping/{}, {}.csv'.format(datetime.date.today(), id_value, output_loop)\n",
    "        while os.path.exists(output):\n",
    "            output_loop += 1\n",
    "            output = './individual scraping/2021-05-06_{}.csv'.format(datetime.date.today(), id_value, output_loop)\n",
    "            \n",
    "        old_dataframe.to_csv(output)\n",
    "        \n",
    "        loop_count = 0 \n",
    "        \n",
    "    else:        \n",
    "        loop_count += 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV data combination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code was combined the individual CSV files. The code was dynamically applied to aggregate CSV files into the segments 1-5, 6-16, 17-36, and 37-50. The code subsequently combined each of the four segments into the complete csv file. The columns were reorganized according to subjective preference with the reference information on the left and the data on the right.\n",
    "\n",
    "Efficiently combining the folder of individually scraped CSV files from the NIST database. The columns are reorganized, and the final CSV is exported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas\n",
    "import numpy\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "\n",
    "#path_original = \"./sabio_scraped/\"\n",
    "path = './individual scraping/'\n",
    "files = glob.glob(os.path.join(path, '*.csv'))\n",
    "\n",
    "# create the total list of dataframes\n",
    "'''print(len(files))'''\n",
    "total_dataframes = []\n",
    "for file in files:\n",
    "    '''re.sub('(\\\\\\)', '(/)', file)\n",
    "    print(file)'''\n",
    "    dfn = pandas.read_csv(file)\n",
    "    total_dataframes.append(dfn)\n",
    "    \n",
    "# combine the total set of dataframes\n",
    "combined_df = pandas.DataFrame()\n",
    "combined_df = pandas.concat(total_dataframes)\n",
    "display(combined_df)\n",
    "\n",
    "# refine the dataframe \n",
    "combined_df = combined_df.fillna(' ')\n",
    "middle_dataframe_columns = ['T(K)', 'pH ', 'K<sub>c</sub>\\' ', 'δ<sub>r</sub>H\\'<sup>o</sup>(kJ.mol<sup>-1</sup>)', 'Km\\'']\n",
    "left_dataframe_columns = ['index', 'Enzyme:', 'Reaction:', 'Reference:', 'Reference ID:'] \n",
    "right_dataframe_columns = list(set(combined_df.columns) - set(left_dataframe_columns) - set(middle_dataframe_columns))\n",
    "defined_columns = left_dataframe_columns + middle_dataframe_columns + right_dataframe_columns  \n",
    "final_dataframe = combined_df.reindex(columns = defined_columns)\n",
    "\n",
    "final_dataframe.to_csv('{}_concatenated scraped NIST enzymes_01.csv'.format(datetime.date.today()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column combination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code combined columns with akin data in the complete CSV. Suffixes were used to further specify data in columns that combined myriad data sources. The columns were finally renamed with generic conventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime \n",
    "import pandas \n",
    "import re\n",
    "\n",
    "empty_cell = ['nan', 'NaN', 'none', 'not given', 'unknown', '0', '', None, ' ']\n",
    "\n",
    "# dataframes definition\n",
    "combined_dataframe = pandas.read_csv(\"2021-05-06_concatenated scraped NIST enzymes_01.csv\")\n",
    "df = combined_dataframe.astype(str)\n",
    "\n",
    "combined_columns = []\n",
    "regex_column_searches = ['(?i)(^K)', '(?i)(^K)', '(ë«|Î\\´|δ)', '(I<sub>c)']\n",
    "base_columns = ['K<sub>c</sub>\\' ', 'K<sub>c</sub>\\' ', 'δ<sub>r</sub>H(cal)/kJ mol<sup>-1</sup>)', 'I<sub>c</sub>(mol dm<sup>-3</sup>)']\n",
    "print('\\nColumns:\\n', '='*len('Columns:'))\n",
    "for this_column in df:\n",
    "    print(this_column)\n",
    "    for index, row in df.iterrows():\n",
    "        for search in range(len(regex_column_searches)):\n",
    "            # combine the like columns\n",
    "            if re.search(regex_column_searches[search], this_column) and not re.search('(Km\\')', this_column):\n",
    "                if str(df.at[index, this_column]) not in empty_cell: \n",
    "                    if str(df.at[index, base_columns[search]]) in empty_cell:\n",
    "                        df.at[index, base_columns[search]] = str(df.at[index, this_column])\n",
    "\n",
    "                    if str(df.at[index, this_column]) not in empty_cell:\n",
    "                        if str(df.at[index, base_columns[search]]) != str(df.at[index, this_column]):\n",
    "                            df.at[index, base_columns[search]] = str(df.at[index, base_columns[search]]) + ' & ' + str(df.at[index, this_column])\n",
    "\n",
    "                if this_column !=  base_columns[search]:\n",
    "                    if this_column not in combined_columns:\n",
    "                        combined_columns.append(this_column)\n",
    "\n",
    "        if re.search('(?<=c\\()(\\w+\\d?\\+?)(?<!,)', this_column):\n",
    "            if re.search('(?<=c\\()(\\w+\\d?\\+?)(?<!,)', this_column):\n",
    "                solute = str(re.search('(?<=c\\()(\\w+\\d?\\+?)(?<!,)', this_column).group(1))\n",
    "                \n",
    "            if str(df.at[index, this_column]) not in empty_cell:\n",
    "                if str(df.at[index, 'c(glycerol,mol dm<sup>-3</sup>)']) in empty_cell:\n",
    "                    df.at[index, 'c(glycerol,mol dm<sup>-3</sup>)'] = str(df.at[index, this_column]) + ' ' + solute\n",
    "\n",
    "                if str(df.at[index, this_column]) not in empty_cell:\n",
    "                    if str(df.at[index, 'c(glycerol,mol dm<sup>-3</sup>)']) != (str(df.at[index, this_column]) or str(df.at[index, this_column]) + ' ' + solute):\n",
    "                        df.at[index, 'c(glycerol,mol dm<sup>-3</sup>)'] = str(df.at[index, 'c(glycerol,mol dm<sup>-3</sup>)']) + ' & ' + str(df.at[index, this_column]) + ' ' + solute\n",
    "\n",
    "            if this_column !=  'c(glycerol,mol dm<sup>-3</sup>)':\n",
    "                if this_column not in combined_columns:\n",
    "                    combined_columns.append(this_column)\n",
    "\n",
    "        if re.search('(?<=m\\()(\\w+\\d?\\+?)(?<!,)', this_column):\n",
    "            if re.search('(?<=c\\()(\\w+\\d?\\+?)(?<!,)', this_column):\n",
    "                solute = str(re.search('(?<=c\\()(\\w+\\d?\\+?)(?<!,)', this_column).group(1))\n",
    "                \n",
    "            if str(df.at[index, this_column]) not in empty_cell:\n",
    "                if str(df.at[index, 'm(MgCl2,mol.kg<sup>-1</sup>)']) in empty_cell:\n",
    "                    df.at[index, 'm(MgCl2,mol.kg<sup>-1</sup>)'] = str(df.at[index, this_column]) + ' ' + solute\n",
    "\n",
    "                if str(df.at[index, this_column]) not in empty_cell:\n",
    "                    if str(df.at[index, 'm(MgCl2,mol.kg<sup>-1</sup>)']) != (str(df.at[index, this_column]) or str(df.at[index, this_column]) + ' ' + solute):\n",
    "                        df.at[index, 'm(MgCl2,mol.kg<sup>-1</sup>)'] = str(df.at[index, 'm(MgCl2,mol.kg<sup>-1</sup>)']) + ' & ' + str(df.at[index, this_column]) + ' ' + solute\n",
    "\n",
    "            if this_column !=  'm(MgCl2,mol.kg<sup>-1</sup>)':\n",
    "                if this_column not in combined_columns:\n",
    "                    combined_columns.append(this_column)\n",
    "\n",
    "        buffer_columns = ['buffer(mol dm<sup>-3</sup>)', 'buffer and/or salt ', 'media ', 'buffer ']\n",
    "        if this_column in buffer_columns:\n",
    "            if str(df.at[index, this_column]) not in empty_cell:\n",
    "                if str(df.at[index, 'Buffer:']) in empty_cell:\n",
    "                    df.at[index, 'Buffer:'] = str(df.at[index, this_column])\n",
    "\n",
    "                if str(df.at[index, 'Buffer:']) not in empty_cell:\n",
    "                    if not re.search(re.escape(str(df.at[index, this_column])), str(df.at[index, 'Buffer:'])):\n",
    "                        df.at[index, 'Buffer:'] = str(df.at[index, 'Buffer:']) + '  +  ' + str(df.at[index, this_column])\n",
    "                        #print('Buffer: ERROR, index: ', index)             \n",
    "\n",
    "            if this_column !=  'Buffer:':\n",
    "                if this_column not in combined_columns:\n",
    "                    combined_columns.append(this_column)\n",
    "\n",
    "        solution_columns = ['salt ', 'cosolvent ', 'added solute ', 'protein ', 'added solute ', 'percent(dimethyl sulfoxide) ', 'p(MPa)', 'pMg ']\n",
    "        if this_column in solution_columns:\n",
    "            if str(df.at[index, this_column]) not in empty_cell:\n",
    "                if str(df.at[index, 'solvent ']) in empty_cell:\n",
    "                    if str(this_column) == 'p(MPa)':\n",
    "                        #print(\"yes: \", index)\n",
    "                        df.at[index, 'solvent '] = str(df.at[index, this_column]) + ' megapascals'  \n",
    "\n",
    "                    elif str(this_column) == 'pMg ':\n",
    "                        #print('yes: ', index)\n",
    "                        df.at[index, 'solvent '] = str(df.at[index, this_column]) + ' = -log[Mg+2]'   \n",
    "                        \n",
    "                    elif str(this_column) == 'percent(dimethyl sulfoxide) ':\n",
    "                        df.at[index, 'solvent '] = str(df.at[index, this_column]) + ' % DMSO'   \n",
    "\n",
    "                    else:\n",
    "                        df.at[index, 'solvent '] = str(df.at[index, this_column])\n",
    "\n",
    "                if str(df.at[index, 'solvent ']) not in empty_cell:\n",
    "                    if not re.search(re.escape(str(df.at[index, this_column])), str(df.at[index, 'solvent '])):\n",
    "                        df.at[index, 'solvent '] = str(df.at[index, 'solvent ']) + '  +  ' + str(df.at[index, this_column])\n",
    "\n",
    "            if this_column !=  'solvent ':\n",
    "                if this_column not in combined_columns:\n",
    "                    combined_columns.append(this_column)\n",
    "            \n",
    "# rename the base columns\n",
    "df.rename(columns = {'c(glycerol,mol dm<sup>-3</sup>)':'solutes [mol / dm^3]', \n",
    "                     'I<sub>c</sub>(mol dm<sup>-3</sup>)':'Ionic strength [mol / dm^3]', \n",
    "                     'T(K)':'T [K]', \n",
    "                     'I<sub>m</sub>(mol.kg<sup>-1</sup>)':'Ionic strength [mol / kg]', \n",
    "                     'm(MgCl2,mol.kg<sup>-1</sup>)':'solutes [mol / kg]', \n",
    "                     'solvent ':'Experimental conditions', \n",
    "                     'K<sub>c</sub>\\' ':'Keq', \n",
    "                     'δ<sub>r</sub>H(cal)/kJ mol<sup>-1</sup>)':'Enthalpy [kJ / mol]',\n",
    "                     'Km\\' ':'Km'},\n",
    "          inplace = True)\n",
    "          \n",
    "print('\\nDeleted columns:')\n",
    "for column in combined_columns:\n",
    "    print(column)\n",
    "    del df[column]\n",
    "    \n",
    "#export total_df to csv \n",
    "df.to_csv(\"{}_vetted & reorganized NIST database_01.csv\".format(datetime.date.today()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing the new and old scraped data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The combined CSV of the NIST scrapped data is contrasted the complete set of IDs. The undescribed references in the recent scraping will be left unscraped, since these references in the NIST databse lack thermodynamic data and are therefore irrelevant, and possibly counterproductive, towards amassing a repository of thermodynamic data through our codes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Undescribed reference:  54GIN/STU_1062\n",
      "Undescribed reference:  54NOD/KUB_593\n",
      "Undescribed reference:  56POD/MOR_1073\n",
      "Undescribed reference:  57WOL/BAL_1173\n",
      "Undescribed reference:  66WOO/DAV_1445\n",
      "Undescribed reference:  67MOR/WHI_596\n",
      "Undescribed reference:  68KOH/WAR_87\n",
      "Undescribed reference:  69BLA_1166\n",
      "Undescribed reference:  74CLA/BIR_1395\n",
      "Undescribed reference:  74FLO/FLE_1063\n",
      "Undescribed reference:  74LAN_648\n",
      "Undescribed reference:  76GOL_548\n",
      "Undescribed reference:  79COR/CRO_16\n",
      "Undescribed reference:  79LAW/VEE_774\n",
      "Undescribed reference:  79LAW/VEE_831\n",
      "Undescribed reference:  84DEM_1065\n",
      "Undescribed reference:  85DEM/BEH_1066\n",
      "Undescribed reference:  86DAL/REN_1067\n",
      "Undescribed reference:  89ROM/DEM_808\n",
      "Undescribed reference:  89ROM/DEM_812\n",
      "Undescribed reference:  91HOR/WAT_468\n",
      "Undescribed reference:  91TEW/GOL2_655\n",
      "Undescribed reference:  91TEW/GOL2_656\n",
      "Undescribed reference:  93LAR/TEW_1053\n",
      "Undescribed reference:  93LAR/TEW_1056\n",
      "Old undescribed reference:  34MEY/LOH_1288\n",
      "Old undescribed reference:  35MEY/LOH_1290\n",
      "Old undescribed reference:  35MEY_1289\n",
      "Old undescribed reference:  43MEY/JUN_1292\n",
      "Old undescribed reference:  54GIN/STU_1062\n",
      "Old undescribed reference:  54NOD/KUB_593\n",
      "Old undescribed reference:  56POD/MOR_1073\n",
      "Old undescribed reference:  57WOL/BAL_1173\n",
      "Old undescribed reference:  66WOO/DAV_1445\n",
      "Old undescribed reference:  67MOR/WHI_596\n",
      "Old undescribed reference:  68KOH/WAR_87\n",
      "Old undescribed reference:  69BLA_1166\n",
      "Old undescribed reference:  74CLA/BIR_1395\n",
      "Old undescribed reference:  74FLO/FLE_1063\n",
      "Old undescribed reference:  74LAN_648\n",
      "Old undescribed reference:  76GOL_548\n",
      "Old undescribed reference:  79COR/CRO_16\n",
      "Old undescribed reference:  79LAW/VEE_774\n",
      "Old undescribed reference:  79LAW/VEE_831\n",
      "Old undescribed reference:  84DEM_1065\n",
      "Old undescribed reference:  85DEM/BEH_1066\n",
      "Old undescribed reference:  86CAS/VEE2_753\n",
      "Old undescribed reference:  86DAL/REN_1067\n",
      "Old undescribed reference:  89ROM/DEM_808\n",
      "Old undescribed reference:  89ROM/DEM_812\n",
      "Old undescribed reference:  91HOR/WAT_468\n",
      "Old undescribed reference:  91TEW/GOL2_655\n",
      "Old undescribed reference:  91TEW/GOL2_656\n",
      "Old undescribed reference:  93LAR/TEW_1053\n",
      "Old undescribed reference:  93LAR/TEW_1056\n",
      "Total reference IDs:  1432\n",
      "Undescribed reference IDs:  25\n",
      "Old undescribed reference IDs:  30\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import bs4\n",
    "\n",
    "final_df = pandas.read_csv('2021-05-06_concatenated scraped NIST enzymes_01.csv')\n",
    "old_df = pandas.read_csv('2021-03-21_vetted + reorganized NIST_1.csv')\n",
    "bs = BeautifulSoup(open('Enzyme Thermodynamic Database.html'), 'lxml')\n",
    "table = bs.find(\"table\", attrs = {'id': 'MainBody_gvSearch'})\n",
    "body = table.find_all(\"tr\")\n",
    "\n",
    "described_ids = final_df['Reference ID:'].tolist()\n",
    "old_described_ids = old_df['Reference ID:'].tolist()\n",
    "reference_ids = []\n",
    "described_reference_ids = []\n",
    "undescribed_reference_ids = []\n",
    "old_undescribed_reference_ids = []\n",
    "for row in range(1, len(body)):\n",
    "    id_value = body[row].find(\"a\").text\n",
    "    id_value = id_value.strip()\n",
    "    reference_ids.append(id_value)   \n",
    "    '''print(id_value)    '''\n",
    "    \n",
    "    if id_value not in described_ids:\n",
    "        undescribed_reference_ids.append(id_value)\n",
    "\n",
    "    if id_value not in old_described_ids:\n",
    "        old_undescribed_reference_ids.append(id_value)\n",
    "        \n",
    "'''undescribed_reference_ids = set(reference_ids) - set(described_reference_ids)'''\n",
    "\n",
    "for id in undescribed_reference_ids:\n",
    "    print('Undescribed reference: ', id)\n",
    "    \n",
    "for id in old_undescribed_reference_ids:\n",
    "    print('Old undescribed reference: ', id)\n",
    "    \n",
    "print('Total reference IDs: ', len(reference_ids))\n",
    "print('Undescribed reference IDs: ', len(undescribed_reference_ids))\n",
    "print('Old undescribed reference IDs: ', len(old_undescribed_reference_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
